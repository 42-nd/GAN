{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEWemWqhLQE_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from math import log2\n",
        "\n",
        "factors = [1,1,1,1,1/2,1/4,1/8,1/16,1/32]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WSConv2d(nn.Module): #weighted-scale a.k.a. eq lr\n",
        "  def __init__(self,in_channels,out_channels,kernel_size = 3, stride =1 , padding = 1, gain = 2):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding)\n",
        "    self.scale = (gain/(in_channels*kernel_size **2)) ** 0.5\n",
        "    self.bias = self.conv.bias # what is it an why?\n",
        "    self.conv.bias = None\n",
        "\n",
        "    #init conv layer\n",
        "    nn.init.normal_(self.conv.weight)#what is it an why?\n",
        "    nn.init.zeros_(self.bias) #what is it an why?\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.conv(x*self.scale) + self.bias.view(1,self.bias.shape[0],1,1) #how it works?"
      ],
      "metadata": {
        "id": "uund5wuGUF-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PixelNorm(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.epsilon = 1e-8\n",
        "\n",
        "  def forward(self,x):\n",
        "    return x / torch.sqrt(torch.mean(x**2,dim=1,keepdim=True) + self.epsilon)"
      ],
      "metadata": {
        "id": "mCU3KyXrUFS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,use_pixelnorm = True):\n",
        "    super().__init__()\n",
        "    self.conv1 = WSConv2d(in_channels,out_channels)\n",
        "    self.conv2 = WSConv2d(out_channels,out_channels)\n",
        "    self.leaky = nn.LeakyReLU(0.2)\n",
        "    self.pn = PixelNorm()\n",
        "    self.use_pn = use_pixelnorm\n",
        "  def forward(self,x):\n",
        "    x = self.leaky(self.conv1(x))\n",
        "    x = self.pn(x) if self.use_pn else x\n",
        "    x = self.leaky(self.conv2(x))\n",
        "    x = self.pn(x) if self.use_pn else x\n",
        "    return x"
      ],
      "metadata": {
        "id": "M32rH6jdW7Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__ (self,z_dim,in_channels,img_channels = 3):\n",
        "    super().__init__()\n",
        "    self.initial = nn.Sequential(\n",
        "        PixelNorm(),\n",
        "        nn.ConvTranspose2d(z_dim,in_channels,4,1,0), #1x1 -> 4x4\n",
        "        nn.LeakyReLU(0.2),\n",
        "        WSConv2d(in_channels,in_channels),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        PixelNorm(),\n",
        "    )\n",
        "\n",
        "    self.initial_rgb = WSConv2d(in_channels,img_channels,kernel_size = 1, stride = 1 , padding = 0)\n",
        "    self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList([self.initial_rgb])\n",
        "\n",
        "    for i in range(len(factors) - 1):\n",
        "      conv_in_channels = int(in_channels * factors[i])\n",
        "      conv_out_channels = int(in_channels * factors[i+1])\n",
        "      self.prog_blocks.append(ConvBlock(conv_in_channels,conv_out_channels))\n",
        "      self.rgb_layers.append(WSConv2d(conv_out_channels,img_channels,kernel_size = 1, stride = 1,padding=0))\n",
        "\n",
        "  def fade_in(self,alpha,upscaled,generated):\n",
        "    return torch.tanh(alpha * generated + (1-alpha) * upscaled)\n",
        "\n",
        "  def forward(self,x,alpha,steps): # steps = 0 (4x4), steps = 1(8x8) ...\n",
        "    out = self.initial(x)\n",
        "\n",
        "    if steps == 0:\n",
        "      return self.initial_rgb(out)\n",
        "\n",
        "    for step in range(steps):\n",
        "      upscaled = F.interpolate(out,scale_factor = 2,mode = \"nearest\") # how it works\n",
        "      out = self.prog_blocks[step](upscaled) # how it works\n",
        "\n",
        "    final_upscaled = self.rgb_layers[steps - 1](upscaled)\n",
        "    final_out = self.rgb_layers[steps](out)\n",
        "\n",
        "    return self.fade_in(alpha,final_upscaled,final_out)"
      ],
      "metadata": {
        "id": "IAo7k7AaW7Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "  def __init__(self, noise_dim,in_channels,img_channels = 3):\n",
        "    super().__init__()\n",
        "    self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList()\n",
        "    self.leaky = nn.LeakyReLU(0.2)\n",
        "\n",
        "    for i in range(len(factors)- 1, 0, -1):\n",
        "      conv_in_channels = int(in_channels * factors[i])\n",
        "      conv_out_channels = int(in_channels * factors[i-1])\n",
        "      self.prog_blocks.append(ConvBlock(conv_in_channels,conv_out_channels,use_pixelnorm = False))\n",
        "      self.rgb_layers.append(WSConv2d(img_channels,conv_in_channels,kernel_size = 1, stride = 1,padding = 0))\n",
        "\n",
        "    self.initial_rgb = WSConv2d(img_channels,in_channels,kernel_size = 1, stride = 1,padding = 0)\n",
        "    self.rgb_layers.append(self.initial_rgb)\n",
        "    self.avg_pool = nn.AvgPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "    self.final_block = nn.Sequential(\n",
        "        WSConv2d(in_channels + 1, in_channels, kernel_size = 3, stride = 1, padding =1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        WSConv2d(in_channels, in_channels, kernel_size = 4, stride = 1, padding =0),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        WSConv2d(in_channels, 1, kernel_size = 1, stride = 1, padding =0),\n",
        "    )\n",
        "\n",
        "  def fade_in(self,alpha,downscaled,out):\n",
        "    return alpha * out + (1-alpha) * downscaled\n",
        "\n",
        "  def minibatch_std(self,x): # need to better understand\n",
        "    batch_statistics = torch.std(x,dim=0).mean().repeat(x.shape[0],1,x.shape[2],x.shape[3])\n",
        "    return torch.cat([x,batch_statistics],dim =1)\n",
        "\n",
        "  def forward(self,x,alpha,steps): # steps = 0 (4x4), steps = 1(8x8) ...\n",
        "    cur_step = len(self.prog_blocks) - steps\n",
        "    out = self.leaky(self.rgb_layers[cur_step](x))\n",
        "\n",
        "    if steps == 0:\n",
        "      out = self.minibatch_std(out)\n",
        "      return self.final_block(out).view(out.shape[0],-1)\n",
        "\n",
        "    downscaled = self.leaky(self.rgb_layers[cur_step +1](self.avg_pool(x)))\n",
        "    out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
        "    out = self.fade_in(alpha,downscaled,out)\n",
        "\n",
        "    for step in range(cur_step+1,len(self.prog_blocks)):\n",
        "      out = self.prog_blocks[step](out)\n",
        "      out = self.avg_pool(out)\n",
        "\n",
        "    out = self.minibatch_std(out)\n",
        "    return self.final_block(out).view(out.shape[0],-1)"
      ],
      "metadata": {
        "id": "okb7GjKtW7is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###CONFIG###\n",
        "import cv2\n",
        "\n",
        "START_TRAIN_AT_IMG_SIZE = 128\n",
        "DATASET = 'celeb_dataset'\n",
        "CHECKPOINT_GEN = \"generator.pth\"\n",
        "CHECKPOINT_CRITIC = \"critic.pth\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SAVE_MODEL = False\n",
        "LOAD_MODEL = False\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZES = [32, 32, 32, 16, 16, 16, 16, 8, 4]\n",
        "CHANNELS_IMG = 3\n",
        "NOISE_DIM = 256  # should be 512 in original paper\n",
        "IN_CHANNELS = 256  # should be 512 in original paper\n",
        "CRITIC_ITERATIONS = 1\n",
        "LAMBDA_GP = 10\n",
        "PROGRESSIVE_EPOCHS = [30] * len(BATCH_SIZES)\n",
        "FIXED_NOISE = torch.randn(8, NOISE_DIM, 1, 1).to(DEVICE)\n",
        "NUM_WORKERS = 4"
      ],
      "metadata": {
        "id": "0ahttvJY2m6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###TEST###\n",
        "gen = Generator(NOISE_DIM, IN_CHANNELS, img_channels=3)\n",
        "critic = Discriminator(NOISE_DIM, IN_CHANNELS, img_channels=3)\n",
        "\n",
        "for img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
        "  num_steps = int(log2(img_size / 4))\n",
        "  x = torch.randn((1,NOISE_DIM,1,1))\n",
        "  z = gen(x, 0.5, steps=num_steps)\n",
        "  assert z.shape == (1, 3, img_size, img_size)\n",
        "  out = critic(z, alpha=0.5, steps=num_steps)\n",
        "  assert out.shape == (1, 1)\n",
        "  print(f\"Success! At img size: {img_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8j_dGR4wk7T",
        "outputId": "248034c3-592b-4b45-85a1-04ddc4e027dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! At img size: 4\n",
            "Success! At img size: 8\n",
            "Success! At img size: 16\n",
            "Success! At img size: 32\n",
            "Success! At img size: 64\n",
            "Success! At img size: 128\n",
            "Success! At img size: 256\n",
            "Success! At img size: 512\n",
            "Success! At img size: 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###UTILS###\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import torchvision\n",
        "from torchvision.utils import save_image\n",
        "from scipy.stats import truncnorm\n",
        "\n",
        "# Print losses occasionally and print to tensorboard\n",
        "def plot_to_tensorboard(writer, loss_critic, loss_gen, real, fake, tensorboard_step):\n",
        "    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # take out (up to) 8 examples to plot\n",
        "        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n",
        "        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n",
        "        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n",
        "        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)\n",
        "\n",
        "\n",
        "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
        "    interpolated_images.requires_grad_(True)\n",
        "\n",
        "    # Calculate critic scores\n",
        "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
        "\n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "def generate_examples(gen, steps, truncation=0.7, n=100):\n",
        "    \"\"\"\n",
        "    Tried using truncation trick here but not sure it actually helped anything, you can\n",
        "    remove it if you like and just sample from torch.randn\n",
        "    \"\"\"\n",
        "    gen.eval()\n",
        "    alpha = 1.0\n",
        "    for i in range(n):\n",
        "        with torch.no_grad():\n",
        "            noise = torch.tensor(truncnorm.rvs(-truncation, truncation, size=(1, NOISE_DIM, 1, 1)), device=DEVICE, dtype=torch.float32)\n",
        "            img = gen(noise, alpha, steps)\n",
        "            save_image(img*0.5+0.5, f\"saved_examples/img_{i}.png\")\n",
        "    gen.train()"
      ],
      "metadata": {
        "id": "o7nxIV0z2176"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###TRAIN###\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.backends.cudnn.benchmarks = True\n",
        "\n",
        "\n",
        "def get_loader(image_size):\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.Normalize(   [0.5 for _ in range(CHANNELS_IMG)],    [0.5 for _ in range(CHANNELS_IMG)],     ),\n",
        "        ]\n",
        "    )\n",
        "    batch_size = BATCH_SIZES[int(log2(image_size / 4))]\n",
        "    dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n",
        "    loader = DataLoader(dataset,batch_size=batch_size,shuffle=True,num_workers=NUM_WORKERS,pin_memory=True,)\n",
        "    return loader, dataset\n",
        "\n",
        "def train_fn(critic,gen,loader,dataset,step,alpha,opt_critic,opt_gen,tensorboard_step,writer,scaler_gen,scaler_critic,):\n",
        "  loop = tqdm(loader, leave = True)\n",
        "  for batch_idx, (real,_) in enumerate(loop):\n",
        "    real = real.to(DEVICE)\n",
        "    cur_batch_size = real.shape[0]\n",
        "\n",
        "    noise = torch.randn(cur_batch_size,NOISE_DIM,1,1).to(DEVICE)\n",
        "\n",
        "    with torch.cuda.amp.autocast(): # need to remember what is autocast\n",
        "      fake = gen(noise,alpha,step)\n",
        "      critic_real = critic(real,alpha,step)\n",
        "      critic_fake = critic(fake.detach(),alpha,step)\n",
        "      gp = gradient_penalty(critic,real,fake,alpha,step,device = DEVICE)\n",
        "      loss_critic =( -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp + (0.001 * torch.mean(critic_real**2)))\n",
        "\n",
        "    opt_critic.zero_grad()\n",
        "    scaler_critic.scale(loss_critic).backward()\n",
        "    scaler_critic.step(opt_critic)\n",
        "    scaler_critic.update()\n",
        "\n",
        "    with torch.cuda.amp.autocast(): # need to remember what is autocast\n",
        "      gen_fake = critic(fake,alpha,step)\n",
        "      loss_gen = -torch.mean(gen_fake)\n",
        "\n",
        "    opt_gen.zero_grad()\n",
        "    scaler_gen.scale(loss_gen).backward()\n",
        "    scaler_gen.step(opt_gen)\n",
        "    scaler_gen.update()\n",
        "\n",
        "    alpha += cur_batch_size/(len(dataset) * PROGRESSIVE_EPOCHS[step] * 0.5)\n",
        "    alpha = min(alpha,1)\n",
        "\n",
        "    if batch_idx % 500 == 0:\n",
        "      with torch.no_grad():\n",
        "        fixed_fakes = gen(FIXED_NOISE, alpha, step) * 0.5 + 0.5\n",
        "      plot_to_tensorboard(\n",
        "        writer,\n",
        "        loss_critic.item(),\n",
        "        loss_gen.item(),\n",
        "        real.detach(),\n",
        "        fixed_fakes.detach(),\n",
        "        tensorboard_step,\n",
        "      )\n",
        "      tensorboard_step += 1\n",
        "\n",
        "      loop.set_postfix(\n",
        "        gp=gp.item(),\n",
        "        loss_critic=loss_critic.item(),\n",
        "      )\n",
        "\n",
        "    return tensorboard_step, alpha\n",
        "def main():\n",
        "  gen = Generator(NOISE_DIM,IN_CHANNELS,img_channels=CHANNELS_IMG).to(DEVICE)\n",
        "  critic = Discriminator(NOISE_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG).to(DEVICE)\n",
        "\n",
        "  opt_gen = optim.Adam(gen.parameters(),lr = LEARNING_RATE,betas = (0.0,0.99))\n",
        "  opt_critic = optim.Adam(critic.parameters(),lr = LEARNING_RATE,betas = (0.0,0.99))\n",
        "\n",
        "  scaler_critic = torch.cuda.amp.GradScaler() #remember what is it\n",
        "  scaler_gen = torch.cuda.amp.GradScaler()\n",
        "\n",
        "  writer = SummaryWriter(f\"logs/gan\")\n",
        "\n",
        "  if LOAD_MODEL:\n",
        "    load_checkpoint(\n",
        "        CHECKPOINT_GEN,gen,opt_gen,LEARNING_RATE,\n",
        "    )\n",
        "    load_checkpoint(\n",
        "        CHECKPOINT_CRITIC,critic,opt_critic,LEARNING_RATE\n",
        "    )\n",
        "\n",
        "  gen.train()\n",
        "  critic.train()\n",
        "  tensorboard_step = 0\n",
        "  step = int(log2(START_TRAIN_AT_IMG_SIZE/4))\n",
        "  for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
        "        alpha = 1e-5\n",
        "        loader, dataset = get_loader(4 * 2 ** step)  # 4->0, 8->1, 16->2, 32->3, 64 -> 4\n",
        "        print(f\"Current image size: {4 * 2 ** step}\")\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "            tensorboard_step, alpha = train_fn(critic,gen,loader,dataset,step,alpha,opt_critic,opt_gen,tensorboard_step,writer,scaler_gen,scaler_critic,)\n",
        "\n",
        "            if SAVE_MODEL:\n",
        "                save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
        "                save_checkpoint(critic, opt_critic, filename=CHECKPOINT_CRITIC)\n",
        "\n",
        "        step += 1  # progress to the next img size"
      ],
      "metadata": {
        "id": "JHvAgAoe3LkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "VTehsb_G4MW8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}